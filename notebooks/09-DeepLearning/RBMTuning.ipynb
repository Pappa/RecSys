{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RBM Hyperparameter Tuning\n",
        "\n",
        "This notebook demonstrates how to tune the hyperparameters of the Restricted Boltzmann Machine (RBM) algorithm using GridSearchCV and compares the performance of tuned vs untuned RBM against random recommendations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup and Imports\n",
        "\n",
        "First, let's import the necessary libraries and set up our random seeds for reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from recsys.MovieLens import MovieLens\n",
        "from RBM import RBM\n",
        "from surprise import NormalPredictor\n",
        "from recsys.Evaluator import Evaluator\n",
        "from surprise.model_selection import GridSearchCV\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Data\n",
        "\n",
        "Load the MovieLens dataset and prepare it for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "lens, ratings_data, rankings = MovieLens.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Grid Search for Best Parameters\n",
        "\n",
        "We'll use GridSearchCV to find the optimal hyperparameters for the RBM algorithm. We'll search over:\n",
        "- Hidden dimensions (10, 20)\n",
        "- Learning rate (0.01, 0.1)\n",
        "\n",
        "The search will use 3-fold cross-validation and evaluate using both RMSE and MAE metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Searching for best parameters...\")\n",
        "param_grid = {\n",
        "    'hidden_dim': [20, 10],\n",
        "    'learning_rate': [0.1, 0.01]\n",
        "}\n",
        "gs = GridSearchCV(RBM, param_grid, measures=['rmse', 'mae'], cv=3)\n",
        "\n",
        "gs.fit(ratings_data)\n",
        "\n",
        "# best RMSE score\n",
        "print(\"Best RMSE score attained: \", gs.best_score['rmse'])\n",
        "\n",
        "# combination of parameters that gave the best RMSE score\n",
        "print(\"Best parameters:\", gs.best_params['rmse'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initialize Evaluator and Add Algorithms\n",
        "\n",
        "Now we'll create an evaluator and add three algorithms to compare:\n",
        "1. RBM with tuned parameters (using the best parameters found by GridSearchCV)\n",
        "2. RBM with default parameters\n",
        "3. Random recommendations (baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluator = Evaluator(ratings_data, rankings)\n",
        "\n",
        "# Add tuned RBM\n",
        "params = gs.best_params['rmse']\n",
        "rbm_tuned = RBM(hidden_dim=params['hidden_dim'], \n",
        "                       learning_rate=params['learning_rate'])\n",
        "evaluator.add_algorithm(rbm_tuned, \"RBM - Tuned\")\n",
        "\n",
        "# Add untuned RBM\n",
        "rbm_untuned = RBM()\n",
        "evaluator.add_algorithm(rbm_untuned, \"RBM - Untuned\")\n",
        "\n",
        "# Add random recommendations as baseline\n",
        "Random = NormalPredictor()\n",
        "evaluator.add_algorithm(Random, \"Random\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = evaluator.evaluate(top_n_metrics=True, coverage_threshold=4.0)\n",
        "\n",
        "results.to_df()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample Recommendations\n",
        "\n",
        "Generate and display some sample recommendations using the evaluated algorithms. \n",
        "\n",
        "This will help us qualitatively assess the differences between tuned and untuned RBM recommendations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "samples = evaluator.sample_top_n_recs(uid=85)\n",
        "\n",
        "for algorithm, recs in samples.items():\n",
        "    print(f\"{algorithm}\")\n",
        "    movie_names = lens.get_movie_names(recs)\n",
        "    for movie_name in movie_names:\n",
        "        print(f\"  {movie_name}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
