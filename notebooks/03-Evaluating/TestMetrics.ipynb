{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluating Recommender System Metrics\n",
        "\n",
        "This notebook demonstrates how to evaluate a recommender system using various metrics including:\n",
        "\n",
        "| Metric    |      |\n",
        "|-----------|------|\n",
        "| RMSE      | Root Mean Squared Error. Lower values mean better accuracy. |\n",
        "| MAE       |  Mean Absolute Error. Lower values mean better accuracy. |\n",
        "| HR        |   Hit Rate; how often we are able to recommend a left-out rating. Higher is better. |\n",
        "| cHR       |  Cumulative Hit Rate; hit rate, confined to ratings above a certain threshold. Higher is better. |\n",
        "| ARHR      | Average Reciprocal Hit Rank - Hit rate that takes the ranking into account. Higher is better. |\n",
        "| Coverage  | Ratio of users for whom recommendations above a certain threshold exist. Higher is better. |\n",
        "| Diversity | 1-S, where S is the average similarity score between every possible pair of recommendations for a given user. Higher means more diverse. |\n",
        "| Novelty   |  Average popularity rank of recommended items. Higher means more novel. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "from recsys.MovieLens import MovieLens\n",
        "from surprise import SVD\n",
        "from surprise import KNNBaseline\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise.model_selection import LeaveOneOut\n",
        "from recsys.RecommenderMetrics import RecommenderMetrics\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preview the ratings data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_types = { 'userId': np.int32, 'movieId': np.int32, 'rating': np.float32, 'timestamp': np.int32 }\n",
        "ratings_df = pd.read_csv('../../src/recsys/data/ratings.csv', dtype=data_types)\n",
        "\n",
        "ratings_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize MovieLens data loader & load data\n",
        "lens, ratings_data, rankings  = MovieLens.load()\n",
        "\n",
        "# Generate item similarities so we can measure diversity later\n",
        "full_trainset = ratings_data.build_full_trainset()\n",
        "knn_options = {'name': 'pearson_baseline', 'user_based': False}\n",
        "similarities_model = KNNBaseline(sim_options=knn_options, verbose=False)\n",
        "similarities_model.fit(full_trainset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train-Test Split and Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainset, testset = train_test_split(ratings_data, test_size=.25, random_state=1)\n",
        "\n",
        "model = SVD(random_state=10, verbose=False)\n",
        "model.fit(trainset)\n",
        "\n",
        "predictions = model.test(testset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate Model Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"RMSE: {RecommenderMetrics.rmse(predictions)}\")\n",
        "print(f\"MAE: {RecommenderMetrics.mae(predictions)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate Top-N Recommendations using Leave-One-Out Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set aside one rating per user for testing\n",
        "loo_iterator = LeaveOneOut(n_splits=1, random_state=1)\n",
        "\n",
        "for trainset, testset in loo_iterator.split(ratings_data):\n",
        "    # Train model without left-out ratings\n",
        "    model.fit(trainset)\n",
        "\n",
        "    # Predicts ratings for left-out ratings only\n",
        "    loo_predictions = model.test(testset)\n",
        "\n",
        "    # Create predictions for all ratings not in the training set\n",
        "    anti_testset = trainset.build_anti_testset()\n",
        "    all_predictions = model.test(anti_testset)\n",
        "\n",
        "    # Calculate top n recommendations for each user\n",
        "    n=10\n",
        "    top_n_predictions = RecommenderMetrics.get_top_n(all_predictions, n=n)\n",
        "\n",
        "    # top_n_predictions2 = RecommenderMetrics.get_top_n2(all_predictions, n=n)\n",
        "\n",
        "    # print(f\"Top-N predictions: {len(top_n_predictions.keys())}\")\n",
        "    # print(f\"Top-N predictions2: {len(top_n_predictions2.keys())}\")\n",
        "\n",
        "    # assert len(top_n_predictions) == len(top_n_predictions2)\n",
        "\n",
        "    # for uid, ratings in top_n_predictions.items():\n",
        "    #     assert len(ratings) == len(top_n_predictions2[uid]), f\"Expected {len(ratings)} but got {len(top_n_predictions2[uid])}\"\n",
        "    #     for i, (iid, rating) in enumerate(ratings):\n",
        "    #         assert iid == top_n_predictions2[uid][i][1], f\"Expected {iid} but got {top_n_predictions2[uid][i][1]}\"\n",
        "    #         assert rating == top_n_predictions2[uid][i][3], f\"Expected {rating} but got {top_n_predictions2[uid][i][3]}\"\n",
        "    \n",
        "    # How often we recommended a movie the user actually rated\n",
        "    # How often we recommended a movie the user actually liked\n",
        "    # ARHR\n",
        "    # Hit Rate by rating value\n",
        "    hit_rate, cumulative_hit_rate, average_reciprocal_hit_rank, rating_hit_rate = RecommenderMetrics.hit_rate_metrics(top_n_predictions, loo_predictions, 4.0)\n",
        "    \n",
        "    print(f\"Hit Rate: {hit_rate:.5f}\")\n",
        "    print(\"rHR (Hit Rate by rating value):\")\n",
        "    for rating, rate in rating_hit_rate:\n",
        "        print(f\"\\t{rating}: {rate:.5f}\")\n",
        "    print(f\"cHR (Cumulative Hit Rate, rating >= 4): {cumulative_hit_rate:.5f}\")\n",
        "    print(f\"ARHR (Average Reciprocal Hit Rank): {average_reciprocal_hit_rank:.5f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate Complete Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(full_trainset)\n",
        "anti_testset = full_trainset.build_anti_testset()\n",
        "all_predictions = model.test(anti_testset)\n",
        "top_n_predictions = RecommenderMetrics.get_top_n(all_predictions, n=10)\n",
        "\n",
        "minimum_rating = 4.0\n",
        "\n",
        "user_coverage = RecommenderMetrics.user_coverage(\n",
        "    top_n_predictions, full_trainset.n_users, minimum_rating=minimum_rating\n",
        ")\n",
        "diversity = RecommenderMetrics.diversity(top_n_predictions, similarities_model)\n",
        "novelty = RecommenderMetrics.novelty(top_n_predictions, rankings)\n",
        "\n",
        "print(f\"User coverage, rating >= {minimum_rating}: {user_coverage:.5f}\")\n",
        "print(f\"Diversity: {diversity:.5f}\")\n",
        "print(f\"Novelty (average popularity rank): {novelty:.5f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
